{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 - PCA and Dimension Reduction Homework\n",
    "Execute the below code and answer the following questions. __Do NOT commit the csv file!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "def generate_data():\n",
    "    x, y = make_classification(n_samples=1500, \n",
    "                            n_features = 20,\n",
    "                            n_informative = 8,\n",
    "                            n_redundant = 5,\n",
    "                            n_repeated = 1, \n",
    "                            n_classes = 3,\n",
    "                            weights = (0.5, 0.25, 0.25),\n",
    "                            random_state = 120\n",
    "                            )\n",
    "    colNames = ['var'+str(x) for x in range(20)]\n",
    "    colNames.append('target')\n",
    "\n",
    "    df = pd.DataFrame(np.concatenate((x,y.reshape(-1,1)), axis=1), columns=colNames)\n",
    "    df.to_csv('pca-dataset.csv', index=False)\n",
    "    \n",
    "generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var0</th>\n",
       "      <th>var1</th>\n",
       "      <th>var2</th>\n",
       "      <th>var3</th>\n",
       "      <th>var4</th>\n",
       "      <th>var5</th>\n",
       "      <th>var6</th>\n",
       "      <th>var7</th>\n",
       "      <th>var8</th>\n",
       "      <th>var9</th>\n",
       "      <th>...</th>\n",
       "      <th>var11</th>\n",
       "      <th>var12</th>\n",
       "      <th>var13</th>\n",
       "      <th>var14</th>\n",
       "      <th>var15</th>\n",
       "      <th>var16</th>\n",
       "      <th>var17</th>\n",
       "      <th>var18</th>\n",
       "      <th>var19</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.882513</td>\n",
       "      <td>-3.272465</td>\n",
       "      <td>-2.520732</td>\n",
       "      <td>-1.987174</td>\n",
       "      <td>-2.073689</td>\n",
       "      <td>-3.272465</td>\n",
       "      <td>-1.237969</td>\n",
       "      <td>1.690547</td>\n",
       "      <td>-0.211314</td>\n",
       "      <td>-5.753190</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.574979</td>\n",
       "      <td>-1.916275</td>\n",
       "      <td>-5.994075</td>\n",
       "      <td>-3.349615</td>\n",
       "      <td>-0.846193</td>\n",
       "      <td>2.491347</td>\n",
       "      <td>1.360958</td>\n",
       "      <td>-2.892522</td>\n",
       "      <td>-1.377561</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.775242</td>\n",
       "      <td>-1.015994</td>\n",
       "      <td>0.005137</td>\n",
       "      <td>0.057274</td>\n",
       "      <td>0.590205</td>\n",
       "      <td>-1.015994</td>\n",
       "      <td>1.350954</td>\n",
       "      <td>-1.493037</td>\n",
       "      <td>-0.862391</td>\n",
       "      <td>-1.986047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.523760</td>\n",
       "      <td>0.399579</td>\n",
       "      <td>0.088600</td>\n",
       "      <td>0.718606</td>\n",
       "      <td>-1.112030</td>\n",
       "      <td>0.083929</td>\n",
       "      <td>0.606544</td>\n",
       "      <td>-1.376793</td>\n",
       "      <td>1.302641</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.876376</td>\n",
       "      <td>0.220453</td>\n",
       "      <td>3.114224</td>\n",
       "      <td>-1.640025</td>\n",
       "      <td>1.180348</td>\n",
       "      <td>0.220453</td>\n",
       "      <td>0.465102</td>\n",
       "      <td>0.222511</td>\n",
       "      <td>0.880455</td>\n",
       "      <td>2.922315</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.370516</td>\n",
       "      <td>3.585262</td>\n",
       "      <td>-2.168162</td>\n",
       "      <td>2.693429</td>\n",
       "      <td>-0.966636</td>\n",
       "      <td>1.586302</td>\n",
       "      <td>-2.821546</td>\n",
       "      <td>0.482164</td>\n",
       "      <td>0.187404</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-2.550342</td>\n",
       "      <td>-1.968144</td>\n",
       "      <td>0.077681</td>\n",
       "      <td>-1.887719</td>\n",
       "      <td>1.864445</td>\n",
       "      <td>-1.968144</td>\n",
       "      <td>-0.527958</td>\n",
       "      <td>-0.201467</td>\n",
       "      <td>-0.532649</td>\n",
       "      <td>2.287445</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041341</td>\n",
       "      <td>2.383582</td>\n",
       "      <td>-0.417253</td>\n",
       "      <td>1.305379</td>\n",
       "      <td>-0.435123</td>\n",
       "      <td>-0.468557</td>\n",
       "      <td>0.923290</td>\n",
       "      <td>3.880050</td>\n",
       "      <td>2.676798</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.454974</td>\n",
       "      <td>1.293300</td>\n",
       "      <td>0.112201</td>\n",
       "      <td>-0.589989</td>\n",
       "      <td>-1.674321</td>\n",
       "      <td>1.293300</td>\n",
       "      <td>0.487302</td>\n",
       "      <td>1.776318</td>\n",
       "      <td>0.702520</td>\n",
       "      <td>-1.024127</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.452869</td>\n",
       "      <td>-0.667306</td>\n",
       "      <td>0.345364</td>\n",
       "      <td>-3.920591</td>\n",
       "      <td>-0.438296</td>\n",
       "      <td>-1.690141</td>\n",
       "      <td>0.176906</td>\n",
       "      <td>1.920142</td>\n",
       "      <td>1.474634</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       var0      var1      var2      var3      var4      var5      var6  \\\n",
       "0 -2.882513 -3.272465 -2.520732 -1.987174 -2.073689 -3.272465 -1.237969   \n",
       "1  0.775242 -1.015994  0.005137  0.057274  0.590205 -1.015994  1.350954   \n",
       "2 -0.876376  0.220453  3.114224 -1.640025  1.180348  0.220453  0.465102   \n",
       "3 -2.550342 -1.968144  0.077681 -1.887719  1.864445 -1.968144 -0.527958   \n",
       "4 -0.454974  1.293300  0.112201 -0.589989 -1.674321  1.293300  0.487302   \n",
       "\n",
       "       var7      var8      var9  ...     var11     var12     var13     var14  \\\n",
       "0  1.690547 -0.211314 -5.753190  ... -0.574979 -1.916275 -5.994075 -3.349615   \n",
       "1 -1.493037 -0.862391 -1.986047  ...  0.523760  0.399579  0.088600  0.718606   \n",
       "2  0.222511  0.880455  2.922315  ... -0.370516  3.585262 -2.168162  2.693429   \n",
       "3 -0.201467 -0.532649  2.287445  ... -0.041341  2.383582 -0.417253  1.305379   \n",
       "4  1.776318  0.702520 -1.024127  ... -0.452869 -0.667306  0.345364 -3.920591   \n",
       "\n",
       "      var15     var16     var17     var18     var19  target  \n",
       "0 -0.846193  2.491347  1.360958 -2.892522 -1.377561     0.0  \n",
       "1 -1.112030  0.083929  0.606544 -1.376793  1.302641     2.0  \n",
       "2 -0.966636  1.586302 -2.821546  0.482164  0.187404     0.0  \n",
       "3 -0.435123 -0.468557  0.923290  3.880050  2.676798     1.0  \n",
       "4 -0.438296 -1.690141  0.176906  1.920142  1.474634     0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('pca-dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1500 entries, 0 to 1499\n",
      "Data columns (total 21 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   var0    1500 non-null   float64\n",
      " 1   var1    1500 non-null   float64\n",
      " 2   var2    1500 non-null   float64\n",
      " 3   var3    1500 non-null   float64\n",
      " 4   var4    1500 non-null   float64\n",
      " 5   var5    1500 non-null   float64\n",
      " 6   var6    1500 non-null   float64\n",
      " 7   var7    1500 non-null   float64\n",
      " 8   var8    1500 non-null   float64\n",
      " 9   var9    1500 non-null   float64\n",
      " 10  var10   1500 non-null   float64\n",
      " 11  var11   1500 non-null   float64\n",
      " 12  var12   1500 non-null   float64\n",
      " 13  var13   1500 non-null   float64\n",
      " 14  var14   1500 non-null   float64\n",
      " 15  var15   1500 non-null   float64\n",
      " 16  var16   1500 non-null   float64\n",
      " 17  var17   1500 non-null   float64\n",
      " 18  var18   1500 non-null   float64\n",
      " 19  var19   1500 non-null   float64\n",
      " 20  target  1500 non-null   float64\n",
      "dtypes: float64(21)\n",
      "memory usage: 246.2 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 1,125\n",
      "Test samples: 375\n",
      "\n",
      "Features:\n",
      "var0\tvar1\tvar2\tvar3\tvar4\tvar5\tvar6\tvar7\tvar8\tvar9\tvar10\tvar11\tvar12\tvar13\tvar14\tvar15\tvar16\tvar17\tvar18\tvar19\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[[x for x in df.columns if x.startswith('var')]]\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=789)\n",
    "\n",
    "print(f'Training samples: {X_train.shape[0]:,}')\n",
    "print(f'Test samples: {X_test.shape[0]:,}')\n",
    "\n",
    "print('\\nFeatures:')\n",
    "print(*X_train, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "- `var1 - var19`: a feature for the data.  \n",
    "- `target`: variable we wish to be able to predict, which is 1 of 3 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "- Use principle components analysis to determine the number of components to reduce the data to by evaluating the explained variance ratio (use `X_train`).  \n",
    "- Remember to scale the data first.  \n",
    "- What number of components would you recommend based on your analysis?  \n",
    "- Explain your results using markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X_train_scaled)\n",
    "\n",
    "plt.plot(range(1, len(X_train.columns)+1), pca.explained_variance_ratio_.cumsum())\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance ratio')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Describe your results here\n",
    "\n",
    "Approximately 80% of the total variance in the dataset appears to be captured by the first 6 principal components, according to the plot of the cumulative explained variance ratio as a function of the number of principal components.\n",
    "\n",
    "Accordingly, if we reduced the dataset's dimensionality to 6 principal components, we would keep the majority of the crucial information while getting rid of some of the noise and redundancy present in the original 19 features.\n",
    "\n",
    "It is important to note that as the number of components rises, the proportion of variation explained by each primary component falls. This is due to the fact that whereas later components capture smaller and more specialized sources of variation, the first few main components capture the greatest sources of variance in the data.\n",
    "\n",
    "The effectiveness of machine learning algorithms may be increased, overfitting can be decreased, and the data can be more easily visualized and interpreted. These are only a few advantages of lowering the dimensionality of the data. The trade-offs between dimensionality reduction and information preservation must be carefully considered, and any models or insights derived from the reduced data must be validated to ensure their validity and applicability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<Insert comments>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "- Evaluate the target variable in the `df` object.  \n",
    "- Which metric would you use in evaluating a predictive model. Explain your choice in the markdown cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.693\n"
     ]
    }
   ],
   "source": [
    "# Insert code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = df[[x for x in df.columns if x.startswith('var')]]\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=789)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Test accuracy: {accuracy:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Describe your results here\n",
    "\n",
    "The particular problem and objectives determine the metric used to assess a predictive model. However, common evaluation metrics accuracy in binary classification issues like this one.\n",
    "In this instance, the model's test accuracy is 0.69, which indicates that in 69.3% of the test cases, the model successfully predicted the target variable. However, it is challenging to assess the suitability of this model based on accuracy alone without knowledge of the class balance and the relative costs of false positives and false negatives. Additional metrics that would give more information about the model's performance include precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "- Without using PCA, create a logistic regression model using practices discussed in class.  \n",
    "- Which model would you choose? Explain your results in the markdown cells.    \n",
    "- What is the accuracy, precision, and recall for the test data?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.693\n",
      "Test precision: 0.695\n",
      "Test recall: 0.672\n"
     ]
    }
   ],
   "source": [
    "# Insert code here\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "X = df[[x for x in df.columns if x.startswith('var')]]\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=789)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Test accuracy: {accuracy:.3f}')\n",
    "print(f'Test precision: {precision:.3f}')\n",
    "print(f'Test recall: {recall:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Describe results here\n",
    "\n",
    "With a test accuracy of 0.693, the logistic regression model without PCA successfully predicted the target variable in 69.3% of the test cases. According to the test precision of 0.695, 69.5% of all the model's positive predictions were in fact true positives. The model accurately detected 67.2% of all actual positive cases, according to the test recall, which was calculated as 0.672.\n",
    "\n",
    "Overall, the model seems to function fairly well in terms of recall, accuracy, and precision. It is challenging to say whether this approach is the best option for this unique circumstance without knowing more about the specific problem and the relative costs of false positives and false negatives.\n",
    "\n",
    "In general, it is crucial to take into account the model's precision, recall, and accuracy when assessing a logistic regression model because these metrics reveal more details about the model's advantages and disadvantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "- Use PCA within a pipeline to create a logistic regression model using best practices from class.  \n",
    "- Which model performs the best on the training data? Explain your results in markdown cells.  \n",
    "- What is the accuracy, precision, and recall for the test data?\n",
    "- Does this perform better than the original logistic regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'model__C': 0.1, 'pca__n_components': 10}\n",
      "Training accuracy: 0.717\n",
      "Test accuracy: 0.688\n",
      "Test precision: 0.692\n",
      "Test recall: 0.666\n"
     ]
    }
   ],
   "source": [
    "# Insert code\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "X = df[[x for x in df.columns if x.startswith('var')]]\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=789)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': range(1, X_train.shape[1] + 1),\n",
    "    'model__C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "print(f'Training accuracy: {grid_search.best_score_:.3f}')\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Test accuracy: {accuracy:.3f}')\n",
    "print(f'Test precision: {precision:.3f}')\n",
    "print(f'Test recall: {recall:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Describe your results here\n",
    "\n",
    "The test accuracy, test precision, and test recall for the pipelined logistic regression model with PCA are 0.688, 0.692, and 0.666 respectively. These numbers are comparable to those from the initial logistic regression model that did not include PCA. The ideal model__C and pca__n_components parameters are 0.1 and 10, respectively, and the training accuracy is 0.717.\n",
    "\n",
    "Overall, the accuracy, precision, and recall of both models are comparable. However, there might be some benefits to the logistic regression model with PCA, such as lowering the dimensionality of the input data and perhaps enhancing model performance on high-dimensional datasets.\n",
    "\n",
    "It is important to note that it is challenging to say with certainty which model is superior to the other without knowing additional specifics about the issue at hand and the relative costs of false positives and false negatives. To properly grasp the models' strengths and shortcomings, it is crucial to take into account a number of evaluation metrics and compare the models on various performance indicators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Without using PCA, create a decision tree model using best practices discussed in class.  \n",
    "- Which model performs the best on the training data? Explain your results in the markdown cells.  \n",
    "- What is the accuracy, precision, and recall for the test data?  \n",
    "- Does this perform better than either of the logistic regression models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 6, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "Training accuracy: 0.748\n",
      "Test accuracy: 0.725\n",
      "Test precision: 0.713\n",
      "Test recall: 0.696\n"
     ]
    }
   ],
   "source": [
    "# Insert code here\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "X = df[[x for x in df.columns if x.startswith('var')]]\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=789)\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=123)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': range(1, X_train.shape[1] + 1),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(tree, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "print(f'Training accuracy: {grid_search.best_score_:.3f}')\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Test accuracy: {accuracy:.3f}')\n",
    "print(f'Test precision: {precision:.3f}')\n",
    "print(f'Test recall: {recall:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Describe your results here\n",
    "\n",
    "The decision tree model without PCA has test accuracy, precision, and recall values of 0.725, 0.713, and 0.696 respectively. The ideal model parameters are'max_depth': 6,'min_samples_leaf': 2,'min_samples_split': 5, and the training accuracy is 0.748.\n",
    "\n",
    "In terms of accuracy, precision, and recall, the decision tree model seems to outperform the logistic regression models. The ability of decision trees to capture non-linear correlations between the input variables and the goal variable, which may be significant in this dataset, may be the reason for this.\n",
    "\n",
    "It is significant to note that the particular dataset and parameters used may have an impact on the decision tree model's performance. Before making a final choice, it may be beneficial to experiment with various hyperparameters and evaluate how various models perform over a range of assessment measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "- Repeat `Question 5` but use PCA.  \n",
    "- Does this perform better than the original Decision Tree or the logistic regression models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'model__max_depth': 7, 'model__min_samples_leaf': 1, 'model__min_samples_split': 2, 'pca__n_components': 15}\n",
      "Training accuracy: 0.742\n",
      "Test accuracy: 0.691\n",
      "Test precision: 0.680\n",
      "Test recall: 0.660\n"
     ]
    }
   ],
   "source": [
    "# Insert code here\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "X = df[[x for x in df.columns if x.startswith('var')]]\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=789)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA()),\n",
    "    ('model', DecisionTreeClassifier(random_state=123))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'pca__n_components': range(1, X_train.shape[1] + 1),\n",
    "    'model__max_depth': range(1, X_train.shape[1] + 1),\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "print(f'Training accuracy: {grid_search.best_score_:.3f}')\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f'Test accuracy: {accuracy:.3f}')\n",
    "print(f'Test precision: {precision:.3f}')\n",
    "print(f'Test recall: {recall:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Describe results here\n",
    "\n",
    "The grid search with cross-validation has identified the best parameters for the Decision Tree Classifier model. With a maximum depth of 7, minimum samples per leaf of 1, minimum samples per split of 2, and 15 PCA components, the model achieved a training accuracy of 0.742.\n",
    "\n",
    "When evaluated on the test set, the model performed with an accuracy of 0.691, precision of 0.680, and recall of 0.660. These findings indicate that the selected parameters and model configuration offer moderate predictive performance on the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
